{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A modular, fully-connected neural network that can be scaled up to have an arbitrary number of hidden layers.\n",
    "\n",
    "# Dependencies: Tensorflow\n",
    "\n",
    "# Features:\n",
    "# - ReLU non-linearities (can easily be swapped out)\n",
    "# - dropout (optional)\n",
    "# - batch normalization (optional but nifty)\n",
    "# - L2 regularization\n",
    "\n",
    "# The architecture of this network will be:\n",
    "# {affine - [batch_norm] - relu - [dropout]} x (L-1) - affine - softmax_loss_function\n",
    "\n",
    "#     L= number of layers\n",
    "\n",
    "#     {...} is repeated (L-1) times\n",
    "\n",
    "# * Affine: no non-linear activation, i.e. just the dot product between input and weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class config(object):\n",
    "#     Parameters of the neural network.\n",
    "\n",
    "#     num_layers: number of hidden layers in the neural network\n",
    "#     hidden_dims: list of integers giving the size of each hidden layer\n",
    "#     input_dims: integer that is the size of the input\n",
    "#     num_classes: number of classes to be classified (i.e. MNIST has 10)\n",
    "#     keep_prob : probabiliy of keeping a neuron; scalar between 0 - 1, if 1 then there is no dropout and you keep all neurons\n",
    "#     use_batch_norm: boolean; whether to use batch normalization\n",
    "    \n",
    "    num_layers= 5\n",
    "    hidden_size=\n",
    "    input_dims = 28 * 28 # MNIST data is 28 x 28 pixels\n",
    "    num_classes = 10 # 10 classes in MNIST\n",
    "    keep_prob=\n",
    "    use_batch_norm = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Cell(object):\n",
    "\n",
    "    def __init__(self, net_params):\n",
    "\n",
    "    def cell(self, config, inputs,keep_prob):\n",
    "        \n",
    "#         This cell will be used in a way that mimics BasicLSTMCell and multirnncell.\n",
    "\n",
    "#         Batch normalization is applied to the input.\n",
    "#         Dropout is applied after the activation function (Relu)\n",
    "\n",
    "\n",
    "#         :param inputs: matrix of inputs\n",
    "#         :param weights: matrix of weighst\n",
    "#         :param biases: matrix of biases\n",
    "#                 keep_prob: float, probability of keeping a neuron\n",
    "#         :return: output matrix (pre-softmax), after dropout has been applied\n",
    "        \n",
    "\n",
    "        # Batch Normalization applied to inputs\n",
    "        batch_mean, batch_variance= tf.nn.moments(inputs, axes=[0,1,2])\n",
    "        gamma = tf.get_variable(tf.ones([config.hidden_size]))\n",
    "        beta = tf.get_variable(tf.zeros([config.hidden_size]))\n",
    "        bn = tf.nn.batch_norm_with_global_normalization(inputs, batch_mean, batch_variance, beta=beta,gamma=gamma,variance_epsilon=1e-3)\n",
    "\n",
    "        # create variable named \"weights\" and \"biases\"\n",
    "\n",
    "        weights = tf.get_variable(\"weights\", [config.hidden_size, config.num_classes],\n",
    "                                  initializer=tf.random_normal_initializer())\n",
    "        biases = tf.get_variable(\"biases\", [config.num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # Dropout applied after Relu\n",
    "        hidden_layer = tf.nn.relu((tf.matmul(bn, weights) + biases)) # Relu applied to logits\n",
    "        h_dropout = tf.nn.dropout(hidden_layer, self.keep_prob) # the predictions\n",
    "\n",
    "        return h_dropout\n",
    "    \n",
    "    def input_size(self, cell):\n",
    "#         calculate the size of the inputs of a cell\n",
    "\n",
    "        #TODO: how to calculate the size of the inputs? is it just tf.size(cell)?\n",
    "\n",
    "    def output_size(self, cell):\n",
    "#         calculate the size of the outputs of a cell\n",
    "        #TODO: how to calculate the size of the outputs?\n",
    "\n",
    "\n",
    "\n",
    "    with tf.variable_scope('neuralnet-'):\n",
    "#         I'm using variable_scope to make it easier to scale up the num_layers\n",
    "#         (which really means scaling up the weights and biases)\n",
    "        \n",
    "\n",
    "        # Create weight variables, using random_normal initializer (this can be changed!)\n",
    "        weights= tf.get_variable('weights',[config.hidden_size, config.input_dims], dtype= tf.float32, initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Create biases, initalize to value of 0\n",
    "        biases = tf.get_variable('biases',config.input_dims, dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_List(NN_Cell):\n",
    "#     Makes a list of num_layer numbered neural net cells that will be used in MultiNNCell\n",
    "\n",
    "    def __init__(self, indivcell, config):\n",
    "        self.indivcell= indivcell= NN_Cell.cell() #todo: Can i just call the cell that was returned by NN_Cell\n",
    "        self.num_layers= num_layers= config.num_layers\n",
    "\n",
    "    def list(self):\n",
    "#         Creating the list of sequential cells; in this version all the cells have the same architecture\n",
    "\n",
    "        self.cell_list= []\n",
    "        self.cell_list.append(self.indivcell * self.num_layers)\n",
    "\n",
    "        return self.cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNNCell (Cell_List):\n",
    "    \"\"\"Neural network composed sequentially of multiple simple cells \"\"\"\n",
    "\n",
    "    def __init__(self, cells):\n",
    "        Create a neural network composed of a number of sequential simple cells\n",
    "\n",
    "        Args:\n",
    "            cells: list of neural net cells that will be\n",
    "        Raises:\n",
    "            ValueError: if cells is empty (not allowed) or if their sizes don't match.\n",
    "        \n",
    "        if not cells:\n",
    "            raise ValueError(\"Must specify at least one cell for MultiNNCell\")\n",
    "        for i in xrange(len(cells) -1):\n",
    "            if cells[i].output_size != cells[i+1].input_size:\n",
    "                raise ValueError(\"In MultiNNCell, the input size of next cell must be same as output size of previous cell\")\n",
    "        self.cells= cells\n",
    "\n",
    "\n",
    "    def sequence(self, inputs):\n",
    "#         Makes the sequence of cells (i.e. the ACTUAL neural network, allowing for the flow of outputs and inputs\n",
    "\n",
    "#         Args:\n",
    "#             inputs: first batch of inputs to be fed into neural network)\n",
    "\n",
    "#         Returns:\n",
    "#             output of neural network, which can be used for softmax cross entropy loss etc.\n",
    "        \n",
    "\n",
    "        # TODO: do i need to use variable scope ?\n",
    "\n",
    "        cur_inp= inputs\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            cur_inp= cell(cur_inp)\n",
    "        return cur_inp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
